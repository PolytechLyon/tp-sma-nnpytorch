{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression linéaire en PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1481, 0.7299],\n",
      "        [0.4962, 0.7306],\n",
      "        [0.0845, 0.9007]])\n",
      "tensor([[0.1481, 0.7299, 0.4962, 0.7306, 0.0845, 0.9007]])\n",
      "tensor([[0.1481, 0.7299, 0.4962],\n",
      "        [0.7306, 0.0845, 0.9007]])\n",
      "tensor([0.5445])\n",
      "torch.Size([1])\n",
      "tensor([0.5445], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "xx = torch.rand(3,2)\n",
    "print(xx)\n",
    "yy = xx.view(1,-1)\n",
    "print(yy)\n",
    "yy.resize_(2,3)\n",
    "print(yy)\n",
    "\n",
    "ax = torch.randn(1)\n",
    "print(ax)\n",
    "print(ax.size())\n",
    "ax.requires_grad_(True)\n",
    "print(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "np.random.seed(40)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1) #  samples from the “standard normal” distribution (mean 0, variance 1)\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "# and cast them into lower precision\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "# Here we can see the difference - notice that .type() is more useful\n",
    "# since it also tells us WHERE the tensor is (device)\n",
    "print(type(x_train), type(x_train_tensor))\n",
    "\n",
    "\n",
    "x_ndarray = x_train_tensor.numpy()\n",
    "print(type(x_ndarray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True)\n",
      "tensor([0.1288], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3367], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Random initialization of parameters\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "a.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.1019])\n",
      "tensor([0.3367], requires_grad=True)\n",
      "tensor([-1.8132])\n",
      "tensor([0.0211])\n",
      "tensor([1.1555], requires_grad=True)\n",
      "tensor([-0.0406])\n",
      "tensor([0.0054])\n",
      "tensor([1.0396], requires_grad=True)\n",
      "tensor([-0.0104])\n",
      "tensor([0.0014])\n",
      "tensor([1.0099], requires_grad=True)\n",
      "tensor([-0.0027])\n",
      "tensor([0.0004])\n",
      "tensor([1.0024], requires_grad=True)\n",
      "tensor([-0.0007])\n",
      "tensor([9.0291e-05])\n",
      "tensor([1.0004], requires_grad=True)\n",
      "tensor([-0.0002])\n",
      "tensor([2.3011e-05])\n",
      "tensor([0.9999], requires_grad=True)\n",
      "tensor([-4.4408e-05])\n",
      "tensor([6.0038e-06])\n",
      "tensor([0.9998], requires_grad=True)\n",
      "tensor([-1.1315e-05])\n",
      "tensor([1.5656e-06])\n",
      "tensor([0.9998], requires_grad=True)\n",
      "tensor([-2.9360e-06])\n",
      "tensor([5.6135e-07])\n",
      "tensor([0.9998], requires_grad=True)\n",
      "tensor([-6.8033e-07])\n",
      "tensor([0.9998], requires_grad=True) tensor([1.9619], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    #print(yhat.grad_fn)\n",
    "    #print(error.grad_fn)\n",
    "    #print(loss.grad_fn)\n",
    "\n",
    "    # No more manual computation of gradients! \n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "    \n",
    "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
    "    loss.backward()\n",
    " \n",
    "    # Let's check the computed gradients...\n",
    "    if epoch % 100 == 0:\n",
    "        print(a.grad)\n",
    "        print(a)\n",
    "        print(b.grad)\n",
    "    \n",
    "    # What about UPDATING the parameters? \n",
    "    \n",
    "    # FIRST ATTEMPT\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    #print(\"dMSE/da avant \",a.grad)\n",
    "    #a = a - lr * a.grad\n",
    "    #b = b - lr * b.grad\n",
    "    #print(\"dMSE/da apres \",a.grad)\n",
    "\n",
    "    #make_dot(yhat).render(\"attached\", format=\"png\")\n",
    "\n",
    "    # SECOND ATTEMPT\n",
    "    # RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n",
    "    #a -= lr * a.grad\n",
    "    #b -= lr * b.grad        \n",
    "    \n",
    "    # THIRD ATTEMPT\n",
    "    # Use NO_GRAD to keep the update out of the gradient computation\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
      "tensor([0.9998], requires_grad=True) tensor([1.9619], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(a, b)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    \n",
    "    # No more manual loss!\n",
    "    # error = y_tensor - yhat\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    " \n",
    "    loss.backward()    \n",
    "    \n",
    "    # No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     a -= lr * a.grad\n",
    "    #     b -= lr * b.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No more telling PyTorch to let gradients go!\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear layer with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call to the layer to make predictions\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[0.7645]])), ('linear.bias', tensor([0.8300]))])\n",
      "OrderedDict([('linear.weight', tensor([[1.9619]])), ('linear.bias', tensor([0.9998]))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Now we can create a model \n",
    "#model = ManualLinearRegression()\n",
    "model = LayerLinearRegression()\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # No more manual prediction!\n",
    "    # yhat = a + b * x_tensor\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
