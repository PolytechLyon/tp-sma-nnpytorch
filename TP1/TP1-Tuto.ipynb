{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel : Réseau de neurones avec PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce tutoriel consiste à construire un réseau de neurones pour résoudre un problème (auparavant) difficile de classification de vêtements à partir d'images. Le dataset Fashion-MNIST est composé d'images de vêtements en niveaux de gris. Chaque image du dataset est de taille 28x28 pixels. \n",
    "\n",
    "<img src='img/fashion-mnist-sprite.png'  width=500px>\n",
    "\n",
    "L'objectif est de construire un réseau de neurones qui prend en entrée une de ses images et prédit quel type de vêtements est dans l'image passée en entrée (parmi 10 classes possibles de vêtements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Fashion-MNIST\n",
    "\n",
    "Tout d'abord, le dataset Fashion-MNIST est récupéré à partir du package `torchvision`.Le code ci-dessous charge le dataset, puis crée les dataset d'entrainement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Une transformation est definie pour normaliser les donness\n",
    "# La tranasformation est appliqué après le chargement des images, avant de les envoyer au NN\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5], [0.5]) \n",
    "                             ])\n",
    "\n",
    "# Chargement des donnees d'entrainement\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Chargement des donnees de test\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données d'entrainement sont chargées dans `trainloader` et un itérateur est disponible avec `iter(trainloader)`. \n",
    "\n",
    "```python\n",
    "dataiter = iter(trainloader)\n",
    "#recuperation d'un batch\n",
    "images, labels = dataiter.next()\n",
    "```\n",
    "\n",
    "Pour boucler sur les données du dataset:\n",
    "\n",
    "```python\n",
    "for images, labels in trainloader:\n",
    "    ## a chaque iteration, recupere un batch d'images dans 'images' et les labels associés\n",
    "```\n",
    "\n",
    ">  <span style=\"color:green\">**Exercice**: </span>:\n",
    "\n",
    "<span style=\"color:green\">- afficher le type et la taille d'un batch</span>\n",
    "\n",
    "<span style=\"color:green\">- à quoi correspondent les différentes dimensions du tenseur `images` ?</span>\n",
    "\n",
    "<span style=\"color:green\">- combien d'images / labels sont contenus dans un batch ?</span>\n",
    "\n",
    "<span style=\"color:green\">- combien de batchs y'a-t'il dans l'ensemble d'entrainement (trainset) et de test (testset) ?</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#affichage de la premiere image du batch\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "plt.imshow(images[0].numpy().squeeze());\n",
    "#The squeeze() function is used to remove single- dimensional entries from the shape of an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappel réseau de neurones\n",
    "\n",
    "Un réseau de neurones est composé d'unités individuelles d'approximation non linéaire, ou neurones. Le réseau de neurones le plus simple est le perceptron mono-couches. Il prend des données en entrées (par ex. 3 entrées ci-dessous), puis réalise une combinaison linéaire : chaque entrée est pondérée par un poids, puis on fait la somme pondérée des entrées (plus le biais). Cette combinaison linéaire est ensuite passée à une fonction d'activation pour calculer la sortie. Les poids $w_i$ sont les paramètres du modèle, mis à jour lors de la remontée de gradient (*backward pass*) pour améliorer la prédiction du réseau.\n",
    "\n",
    "<img src=\"img/perceptronmono.png\" width=600px>\n",
    "\n",
    "D'un point de vue mathématique, le perceptron mono-couche ci-dessus calcule (prédiction - *forward pass*):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + w_3 x_3 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Les fonctions d'activation introduisent la non-linéarité dans le réseau de neurones. Différentes fonctions d'activation peuvent être choisies:\n",
    "\n",
    "<img src=\"img/activation.png\" width=700px>\n",
    "\n",
    "* L'une des premières fonction d'activation a été la fonction *sigmoïd* aussi connue sous le nom de *fonction logistique*. Elle présente cependant quelques inconvénients, en particulier, le gradient (dérivée) à ses extrémités (0 ou 1) est nul. A cause de cette limitation, le modèle peut souffrir de ce que l'on appelle le **vanishing gradient**, qui signifie que le gradient de la fonction diminue progressivement (peut même devenir nul) et l'apprentissage devient moins efficace car les paramètres ne sont plus mis à jour correctement. \n",
    "* Une autre fonction d'activation qui marche presque toujours mieux que la fonction logistique est la fonction *tangente hyperbolique*, dont les valeurs de sortie sont centrée en 0 (contrairement à sigmoid). Mais elle présente aussi le problème du vanishing gradient.  \n",
    "* La fonction d'activation la plus utilisée de nos jours est la **ReLU - Rectified Linear Unit -** qui est une fonction linéaire par morceaux. Son avantage réside sur le fait qu'elle remplace toute valeur d'entrée négative par 0 et toute valeur positive par elle même (ReLU réalise la fonciton  $max(0,x)$). De plus, son gradient devient nul pour les valeurs négatives et vaut 1 pour les valeurs positives. Cette propriété est très intéressante dans la phase d'apprentissage car elle permet d'éviter et de corriger le problème du vanishing gradient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un perceptron multi-couches, ou modèle *feed-forward*, plusieurs couches de neurones sont accumulées, où chaque neurone d'une couche est connecté à chaque neurone de la couche suivante. Ci-dessous, un perceptron multi-couches avec 4 entrées et 3 sorties.\n",
    "\n",
    "<img src=\"img/mlp.png\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple: Construction d'un réseau de neurones avec PyTorch\n",
    "\n",
    "Pour construire un réseau de neurones, nous utilisons le module [torch.nn](https://pytorch.org/docs/stable/nn.html) et héritons de la classe `torch.nn.Module`.\n",
    "\n",
    "* Dans le constructeur `init`, les paramètres du modèle sont définis. Ces paramètres sont les poids de chaque couche, i.e. des tenseurs sur lesquels une remontée de gradient devra être réalisée lors de l'apprentissage du réseau. Pour cela, on peut utiliser les *layers* de PyTorch. Ainsi, chaque couche du réseau est définie séparément. Pour un réseau *feed-forward* où chaque couche réalise une combinaison linéaire, on utilise le *layer* `nn.Linear(nbx,nby)` où `nbx` est le nombre d'entrées et `nby` le nombre de sorties de la couche. \n",
    "\n",
    "* La méthode `forward(x)` réalise la prédiction (*forward pass*) du réseau pour une entrée `x`. Il faut donc réaliser les prédictions de chaque couche du modèle, et les passer aux fonctions d'activation. Pour les fonctions d'activation, le module `torch.nn.functional`  est couramment importé sous le nom `F`. Pour une activation ReLU par exemple, `F.relu(x)` calcule l'activation ReLU pour le tenseur `x`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le perceptron mono-couche donné en exemple précédemment est modélisé ci-dessous en PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronMonoCouche(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #bonne pratique: nommer vos couches selon leur type, par exemple `fc` pour une couche fully-connected.\n",
    "        self.fc1 = nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "\n",
    "        x = self.fc1(x) #prédiction de la couche linéaire fc1\n",
    "        x = F.relu(x) # application de la fonction d'activation ReLU\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On instancie ensuite une variable `model`de type `PerceptronMonoCouche`, et on peut afficher les poids et biais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poids init auto:  Parameter containing:\n",
      "tensor([[-0.5335, -0.3580,  0.0070]], requires_grad=True)\n",
      "biais init auto: Parameter containing:\n",
      "tensor([-0.3805], requires_grad=True)\n",
      "biais init 0: Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "poids init random:  Parameter containing:\n",
      "tensor([[-0.0043, -0.0041,  0.0139]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = PerceptronMonoCouche()\n",
    "# affichage des poids et biais de la couche fc1 du modèle (automatiquement initialisé)\n",
    "print(\"poids init auto: \",model.fc1.weight)\n",
    "print(\"biais init auto:\" ,model.fc1.bias)\n",
    "\n",
    "# Init des biais à 0\n",
    "model.fc1.bias.data.fill_(0)\n",
    "print(\"biais init 0:\" ,model.fc1.bias)\n",
    "\n",
    "# Init des poids selon random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)\n",
    "print(\"poids init random: \",model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Données d'entrée du réseau (batch)\n",
    "\n",
    "On peut calculer la prédiction du modèle (*forward pass*) pour une **unique** donnée d'entrée `x` qui est un tenseur 1D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([0.6270, 0.9826, 0.4502]) torch.Size([3])\n",
      "prediction du modele PerceptronMonoCouche:  tensor([0.], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3)\n",
    "print(\"input\",x, x.size())\n",
    "out = model(x)\n",
    "print(\"prediction du modele PerceptronMonoCouche: \",out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi passer au réseau les données d'un batch, sous forme d'un **tenseur 2D** de taille (taille du batch, taille de l'entrée). Le réseau réalisera les prédictions pour chaque élément du batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch tensor([[0.6036, 0.5347, 0.7587],\n",
      "        [0.3976, 0.8534, 0.2969],\n",
      "        [0.0626, 0.5036, 0.7537],\n",
      "        [0.6159, 0.4587, 0.7256],\n",
      "        [0.7816, 0.4108, 0.4089],\n",
      "        [0.7255, 0.1706, 0.0016],\n",
      "        [0.6875, 0.4760, 0.1117],\n",
      "        [0.6577, 0.0812, 0.0401],\n",
      "        [0.8768, 0.9263, 0.1832],\n",
      "        [0.1109, 0.9972, 0.3207]]) torch.Size([10, 3])\n",
      "prediction du modele PerceptronMonoCouche pour le batch:  tensor([[0.0057],\n",
      "        [0.0000],\n",
      "        [0.0081],\n",
      "        [0.0055],\n",
      "        [0.0006],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#batch de taille 10\n",
    "xbatch = torch.rand(10,3)\n",
    "print(\"input batch\",xbatch,xbatch.size())\n",
    "out = model(xbatch)\n",
    "print(\"prediction du modele PerceptronMonoCouche pour le batch: \",out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Construction du réseau de neurones pour Fashion-MNIST\n",
    "\n",
    "Pour concevoir le réseau *feed forward* pour traiter les données du dataset MNIST, il faut :\n",
    "* connaitre la taille des données d'entrées: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Exercice**: </span>:\n",
    "<span style=\"color:green\">- étant donné la taille des images du dataset, quelle sera le nombre d'entrées de votre NN ?</span>\n",
    "\n",
    "<span style=\"color:green\">- récupérez une image du dataset, et applatissez-la pour pouvoir passer cette image sous forme de Tenseur 1D à votre réseau.</span>\n",
    "\n",
    "<span style=\"color:green\">- étant donné la taille des images du dataset et la taille des batchs, quelle sera la dimension du tenseur 2D passé à votre réseau de neurones dans le cas où l'on passe au réseau un batch d'images ?</span>\n",
    "\n",
    "<span style=\"color:green\">-  récupérez un batch du dataset, et traiter ce batch pour pouvoir le passer sous forme de Tenseur 2D à votre réseau.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* connaitre la taille de la couche de sortie: le problème est ici un problème de classification en 10 classes (10 types de vêtements). La taille de la sortie du réseau sera donc `10`, 1 sortie par classe. \n",
    "\n",
    "* décider des fonctions d'activation à appliquer: pour les couches cachées, la fonction d'activation ReLU est choisie.\n",
    "\n",
    "* pour la couche de sortie,  on souhaite que la `i`-ème sortie calcule la probabilité que l'image passée  en entrée soit de la classe `i`. Pour convertir la sortie du réseau en distribution de probabilité, la fonction d'activation [**softmax**](https://en.wikipedia.org/wiki/Softmax_function) peut être utilisée car elle permet de *formater* les 10 valeurs de sortie pour qu'elles soient entre 0 et 1 avec une somme égale à 1. La [log-probability](https://en.wikipedia.org/wiki/Log_probability), qui est une fonction qui calcule le logarithme de la fonction softmax, est à préférer car elle apporte différents avantages: les calculs sont souvent plus rapide et plus précis (cf. illustration [ici](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)). Pour récupérer les probabilités à partir de la log-probabilité, il suffit de prendre l'exponentiel (`torch.exp`), en effet:\n",
    "$$ \\large{e^{\\ln{x}} = x }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous proposons de construire pour débuter le réseau suivant (avec `log_softmax` en sortie et pas `softmax`):\n",
    "\n",
    "<img src=\"img/mlp_mnist2.png\" width=600px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> <span style=\"color:green\">**Exercice**: Construisez ce réseau. Instanciez le dans une variable `model`. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerceptronMonoCouche(\n",
      "  (fc1): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "\n",
    "On va afficher les prédictions du modèle (probabilité de chaque classe) en passant une unique image au réseau:\n",
    "\n",
    "> <span style=\"color:green\">**Exercice:** Compléter le code ci-dessous</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exp(): argument 'input' (position 1) must be Tensor, not ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-39e964e85517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Calcul des proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Visualisation des prédictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: exp(): argument 'input' (position 1) must be Tensor, not ellipsis"
     ]
    }
   ],
   "source": [
    "#  TODO:  Récupération d'une image d'un batch et forward pass du modele\n",
    "output = ...\n",
    "\n",
    "\n",
    "\n",
    "# Calcul des probabilités pour l' image \n",
    "probs = torch.exp(output)\n",
    "print(probs)\n",
    "\n",
    "# Visualisation des probabilités/prédictions pour l'image'\n",
    "helper.view_classify(img.view(1, 28, 28), probs, version='Fashion') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  <span style=\"color:green\">**Exercice:** Que pouvez-vous en conclure ?</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant afficher les prédictions du modèle (probabilité de chaque classe) en passant un batch d'images au réseau:\n",
    "\n",
    "> <span style=\"color:green\">**Exercice:** Compléter le code ci-dessous</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO:  Récupération d'un batch et forward pass du modele\n",
    "output = ...\n",
    "\n",
    "\n",
    "\n",
    "# Calcul des probabilités pour chaque image du batch\n",
    "probs = torch.exp(output)\n",
    "print(probs)\n",
    "\n",
    "# Visualisation des probabilités/prédictions pour la première image du batch\n",
    "helper.view_classify(img.view(1, 28, 28), probs[0], version='Fashion') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Entrainement du NN\n",
    "\n",
    "Pour entrainer le réseau de neurones, nous devons :\n",
    "*  définir la fonction de perte à utiliser, dans notre cas, la fonction de perte [**Logarithme Négatif**](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) est adapté à notre problème de classification et à la sortie log-softmax du réseau.\n",
    "\n",
    "* définir l'optimiseur à utiliser, qui se chargera de mettre à jour les paramètres/poids du réseau. Dans notre cas, nous pouvons utiliser la descente de gradient stochastique (SGD) avec `torch.optim.SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1) # lr est le coefficient d'apprentissage de la descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement pas à pas\n",
    "\n",
    "Nous allons tout d'abord considérer *une seule itération d'apprentissage*, i.e. une seule mise à jour des paramètres du NN en utilisant un mini-batch. Les 4 étapes pour la mise à jour des paramètres sont les suivantes:\n",
    "\n",
    "* *forward pass*: calcul des prédictions du modèle \n",
    "* calcul de la *loss*\n",
    "* *backward pass*: calcul du gradient \n",
    "* mise à jour des paramètres (poids du réseau)\n",
    "\n",
    ">   <span style=\"color:green\">**Exercice:** Implémentez ci-dessous une itération de descente de gradient sur un mini-batch. Quelle est la taille du Tenseur renvoyé en sortie par votre NN lors d'une *forward pass* ?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids initiaux -  Parameter containing:\n",
      "tensor([[-0.0032,  0.0040,  0.0137]], requires_grad=True)\n",
      "Gradient - None\n",
      "Poids mis à jour -  Parameter containing:\n",
      "tensor([[-0.0032,  0.0040,  0.0137]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Poids initiaux - ', model.fc1.weight)\n",
    "\n",
    "\n",
    "#TODO\n",
    "\n",
    "\n",
    "\n",
    "##Apres une backward pass\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "print('Poids mis à jour - ', model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement itératif sur plusieurs *epochs*\n",
    "\n",
    "> <span style=\"color:green\">**Exercice:** </span>\n",
    "    \n",
    "<span style=\"color:green\"> - Combien y aura-t-il d'itérations (mise à jour des paramètres du NN) par epoch ? </span>\n",
    "    \n",
    "<span style=\"color:green\"> - Implémentez ci-dessous l'apprentissage complet du réseau de neurones sur plusieurs **epochs**. Vous afficherez toutes les 40 itérations:</span>\n",
    ">\n",
    "<span style=\"color:green\">* le numéro de l'epoch</span>\n",
    "\n",
    "<span style=\"color:green\">* la perte d'entrainement, qui est la somme des pertes sur ces 40 itérations divisé par 40</span>\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repet: 1/5..  affiche:: 0.200.. \n",
      "repet: 2/5..  affiche:: 1.200.. \n",
      "repet: 3/5..  affiche:: 2.200.. \n",
      "repet: 4/5..  affiche:: 3.200.. \n",
      "repet: 5/5..  affiche:: 4.200.. \n"
     ]
    }
   ],
   "source": [
    "# exemple pour un affichage dans une boucle\n",
    "repets = 5\n",
    "for i in range(repets):\n",
    "    print(\"repet: {}/{}.. \".format(i+1, repets),\n",
    "         \"affiche:: {:.3f}.. \".format(i+1/repets))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des prédictions sur une image\n",
    "\n",
    "> <span style=\"color:green\">**Exercice:** Compléter le code ci-dessous et tester. Conclusion ?  </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exp(): argument 'input' (position 1) must be Tensor, not ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-550672bc6846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Calcul des proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Visualisation des prédictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: exp(): argument 'input' (position 1) must be Tensor, not ellipsis"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO\n",
    "\n",
    "\n",
    "\n",
    "# Calcul des proba\n",
    "probs = torch.exp(output)\n",
    "\n",
    "# Visualisation des prédictions\n",
    "helper.view_classify(img.resize_(1, 28, 28), probs, version='Fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test du réseau et calcul de la précision (*accuracy*)\n",
    "\n",
    "Maintenant que le réseau a été entrainé, nous pouvons tester ses performances. Pour cela, nous pouvons utiliser la perte (*loss*) et/ou la précision (*accuracy*) du réseau, qui correspond au pourcentage de prédictions justes sur l'ensemble des prédictions réalisées.\n",
    "\n",
    "* La précision sur les données d'entrainement permet d'évaluer le sous-apprentissage (*underfit*). Si la précision sur les données d'entrainement est faible, le modèle échoue à apprendre la fonction de prédiction. Dans ce cas, les solutions peuvent être de modifier l'architecture du réseau (par ex. pour un réseau plus grand) ou d'entrainer le réseau plus longtemps. \n",
    "\n",
    "* La précision sur les données de test permet de vérifier si le réseau est capable de **généraliser**, i.e. de réaliser des prédictions sur des données qu'il n'a jamais vu. Si la précision sur les données de test est faible (et que la précision sur les données d'entrainement est élevée), le réseau éhcoue à généraliser (sur-apprentissage, *overfit*).  Dans ce cas, les solutions peuvent être de d'utiliser plus de données d'entrainement, de diminuer le nombre de paramètres, ou de faire de la [**régularisation**](https://medium.com/yottabytes/a-quick-guide-on-basic-regularization-methods-for-neural-networks-e10feb101328), par ex. le *dropout*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  <span style=\"color:green\">**Exercice**: Calculer et afficher ci-dessous la précision de votre réseau sur les données d'entrainement et sur les données de test.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5d7941aa86a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# affichage des précision (en pourcentage)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m print('Précision du réseau sur les %d images d entrainement: %d %%' % (total_train,\n\u001b[0m\u001b[1;32m      5\u001b[0m     accuracy_train))\n\u001b[1;32m      6\u001b[0m print('Précision du réseau sur les %d images de test: %d %%' % (total_test,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_train' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "# affichage des précision (en pourcentage)\n",
    "print('Précision du réseau sur les %d images d entrainement: %d %%' % (total_train,\n",
    "    accuracy_train))\n",
    "print('Précision du réseau sur les %d images de test: %d %%' % (total_test,\n",
    "    accuracy_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde d'un réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est utile de pouvoir sauvegarder un réseau entrainé pour le recharger plus tard (afin de plus l'entrainer, ou de l'utiliser pour faire des prédictions).  Les paramètres d'un réseau PyTorch sont dans `state_dict`. Cette variable contient les poids et biais de chaque couche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " PerceptronMonoCouche(\n",
      "  (fc1): Linear(in_features=3, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['fc1.weight', 'fc1.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le plus simple pour sauvegarder un réseau est de sauvegarder `state_dict` avec `torch.save`, par exemple dans un fichier `checkpoint.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ensuite charger  `state_dict` avec `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['fc1.weight', 'fc1.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode fonctionne si l'architecture du modèle dans lequel on chargera les poids et biais est identique à celle du réseau sauvegardé. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerceptronMonoCouche(\n",
      "  (fc1): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = PerceptronMonoCouche()\n",
    "model.load_state_dict(state_dict)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Compte-rendu\n",
    "\n",
    "Vous pouvez maintenant modifier des éléments pour améliorer la précision de votre réseau si besoin !\n",
    "\n",
    "> Exercice: Préciser ci-dessous les difficultés rencontrés pendant ce TP, les précisions d'entrainement et de test que vous avez obtenu avec le réseau proposé, et les modifications que vous avez peut être testées pour améliorer l'apprentissage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
