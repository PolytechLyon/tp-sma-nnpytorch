{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel : Réseau de neurones avec PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce tutoriel consiste à construire un réseau de neurones pour résoudre un problème (auparavant) difficile de classification de vêtements à partir d'images. Le dataset Fashion-MNIST est composé d'images de vêtements en niveaux de gris. Chaque image du dataset est de taille 28x28 pixels. \n",
    "\n",
    "<img src='img/fashion-mnist-sprite.png'  width=500px>\n",
    "\n",
    "L'objectif est de construire un réseau de neurones qui prend en entrée une de ses images et prédit quel type de vêtements est dans l'image passée en entrée (parmi 10 classes possibles de vêtements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Fashion-MNIST\n",
    "\n",
    "Tout d'abord, le dataset Fashion-MNIST est récupéré à partir du package `torchvision`.Le code ci-dessous charge le dataset, puis crée les dataset d'entrainement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to F_MNIST_data/FashionMNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159.1%/Users/laetitiamatignon/anaconda3/envs/deeprlTP/lib/python3.6/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1595629444482/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to F_MNIST_data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Une transformation est definie pour normaliser les donness\n",
    "# La tranasformation est appliqué après le chargement des images, avant de les envoyer au NN\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5], [0.5]) \n",
    "                             ])\n",
    "\n",
    "# Chargement des donnees d'entrainement\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Chargement des donnees de test\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données d'entrainement sont chargées dans `trainloader` et un itérateur est disponible avec `iter(trainloader)`. Pour boucler sur les données du dataset:\n",
    "\n",
    "```python\n",
    "for image, label in trainloader:\n",
    "    ## do things with images and labels\n",
    "```\n",
    "\n",
    "`trainloader` est créé avec une taille de batch de 64, i.e. que **64 images sont récupérées à chaque itération**. Ces 64 images, ou *batch*, seront envoyées en entrée du réseau. `shuffle=True` permet de mélanger le dataset à chaque appel du `dataloader`.\n",
    "\n",
    "Ci-dessous, on récupère un batch dans `images` qui est donc un tenseur de taille  `(64, 1, 28, 28)`, soit 64 images par batch, 1 color channel, et la taille des images 28x28.\n",
    "\n",
    "> **Exercice**: afficher ci-dessous le type et la taille d'un batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recuperation d'un batch\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#affichage de la premiere image du batch\n",
    "plt.imshow(images[0].numpy().squeeze());\n",
    "#The squeeze() function is used to remove single- dimensional entries from the shape of an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappel réseau de neurones\n",
    "\n",
    "Un réseau de neurones est composé d'unités individuelles d'approximation non linéaire, ou neurones. Le réseau de neurones le plus simple est le perceptron mono-couches. Il prend des données en entrées (par ex. 3 entrées ci-dessous), puis réalise une combinaison linéaire : chaque entrée est pondérée par un poids, puis on fait la somme pondérée des entrées (plus le biais). Cette combinaison linéaire est ensuite passée à une fonction d'activation pour calculer la sortie. Les poids $w_i$ sont les paramètres du modèle, mis à jour lors de la remontée de gradient (*backward pass*) pour améliorer la prédiction du réseau.\n",
    "\n",
    "<img src=\"img/perceptronmono.png\" width=600px>\n",
    "\n",
    "D'un point de vue mathématique, le perceptron mono-couche ci-dessus calcule (prédiction - *forward pass*):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + w_3 x_3 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Les fonctions d'activation introduisent la non-linéarité dans le réseau de neurones. Différentes fonctions d'activation peuvent être choisies:\n",
    "\n",
    "<img src=\"img/activation.png\" width=700px>\n",
    "\n",
    "* L'une des premières fonction d'activation a été la fonction *sigmoïd* aussi connue sous le nom de *fonction logistique*. Elle présente cependant quelques inconvénients, en particulier, le gradient (dérivée) à ses extrémités (0 ou 1) est nul. A cause de cette limitation, le modèle peut souffrir de ce que l'on appelle le **vanishing gradient**, qui signifie que le gradient de la fonction diminue progressivement (peut même devenir nul) et l'apprentissage devient moins efficace car les paramètres ne sont plus mis à jour correctement. \n",
    "* Une autre fonction d'activation qui marche presque toujours mieux que la fonction logistique est la fonction *tangente hyperbolique*, dont les valeurs de sortie sont centrée en 0 (contrairement à sigmoid). Mais elle présente aussi le problème du vanishing gradient.  \n",
    "* La fonction d'activation la plus utilisée de nos jours est la **ReLU - Rectified Linear Unit -** qui est une fonction linéaire par morceaux. Son avantage réside sur le fait qu'elle remplace toute valeur d'entrée négative par 0 et toute valeur positive par elle même (ReLU réalise la fonciton  $max(0,x)$). De plus, son gradient devient nul pour les valeurs négatives et vaut 1 pour les valeurs positives. Cette propriété est très intéressante dans la phase d'apprentissage car elle permet d'éviter et de corriger le problème du vanishing gradient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un perceptron multi-couches, ou modèle *feed-forward*, plusieurs couches de neurones sont accumulées, où chaque neurone d'une couche est connecté à chaque neurone de la couche suivante. Ci-dessous, un perceptron multi-couches avec 4 entrées et 3 sorties.\n",
    "\n",
    "<img src=\"img/mlp.png\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple: Construction d'un réseau de neurones avec PyTorch\n",
    "\n",
    "Pour construire un réseau de neurones, nous utilisons le module [torch.nn](https://pytorch.org/docs/stable/nn.html) et héritons de la classe `torch.nn.Module`.\n",
    "\n",
    "* Dans le constructeur `init`, les paramètres du modèle sont définis. Ces paramètres sont les poids de chaque couche, i.e. des tenseurs sur lesquels une remontée de gradient devra être réalisée lors de l'apprentissage du réseau. Pour cela, on peut utiliser les *layers* de PyTorch. Ainsi, chaque couche du réseau est définie séparément. Pour un réseau *feed-forward* où chaque couche réalise une combinaison linéaire, on utilise le *layer* `nn.Linear(nbx,nby)` où `nbx` est le nombre d'entrées et `nby` le nombre de sorties de la couche. \n",
    "\n",
    "* La méthode `forward(x)` réalise la prédiction (*forward pass*) du réseau pour une entrée `x`. Il faut donc réaliser les prédictions de chaque couche du modèle, et les passer aux fonctions d'activation. Pour les fonctions d'activation, le module `torch.nn.functional`  est couramment importé sous le nom `F`. Pour une activation ReLU par exemple, `F.relu(x)` calcule l'activation ReLU pour le tenseur `x`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le perceptron mono-couche donné en exemple précédemment est modélisé ci-dessous en PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronMonoCouche(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #bonne pratique: nommer vos couches selon leur type, par exemple `fc` pour une couche fully-connected.\n",
    "        self.fc1 = nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "\n",
    "        x = self.fc1(x) #prédiction de la couche linéaire fc1\n",
    "        x = F.relu(x) # application de la fonction d'activation ReLU\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On instancie ensuite une variable `model`de type `PerceptronMonoCouche`, et on peut afficher les poids et biais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poids init auto:  Parameter containing:\n",
      "tensor([[ 0.5168,  0.2285, -0.0417]], requires_grad=True)\n",
      "biais init auto: Parameter containing:\n",
      "tensor([-0.1621], requires_grad=True)\n",
      "biais init 0: Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "poids init random:  Parameter containing:\n",
      "tensor([[-0.0032,  0.0040,  0.0137]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = PerceptronMonoCouche()\n",
    "# affichage des poids et biais de la couche fc1 du modèle (automatiquement initialisé)\n",
    "print(\"poids init auto: \",model.fc1.weight)\n",
    "print(\"biais init auto:\" ,model.fc1.bias)\n",
    "\n",
    "# Init des biais à 0\n",
    "model.fc1.bias.data.fill_(0)\n",
    "print(\"biais init 0:\" ,model.fc1.bias)\n",
    "\n",
    "# Init des poids selon random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)\n",
    "print(\"poids init random: \",model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Données d'entrée du réseau (batch)\n",
    "\n",
    "On peut calculer la prédiction du modèle (*forward pass*) pour une **unique** donnée d'entrée `x` qui est un tenseur 1D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([0.5802, 0.2034, 0.6309]) torch.Size([3])\n",
      "prediction du modele PerceptronMonoCouche:  tensor([0.0076], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3)\n",
    "print(\"input\",x, x.size())\n",
    "out = model(x)\n",
    "print(\"prediction du modele PerceptronMonoCouche: \",out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi passer au réseau les données d'un batch, sous forme d'un **tenseur 2D** de taille (taille du batch, taille de l'entrée). Le réseau réalisera les prédictions pour chaque élément du batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch tensor([[0.1788, 0.0696, 0.7768],\n",
      "        [0.8025, 0.2738, 0.7749],\n",
      "        [0.1583, 0.9450, 0.0882],\n",
      "        [0.3875, 0.2027, 0.9338],\n",
      "        [0.0667, 0.1550, 0.1618],\n",
      "        [0.6405, 0.9410, 0.7455],\n",
      "        [0.0412, 0.4330, 0.6001],\n",
      "        [0.9408, 0.6775, 0.3296],\n",
      "        [0.2197, 0.7348, 0.8378],\n",
      "        [0.2211, 0.9318, 0.8706]]) torch.Size([10, 3])\n",
      "prediction du modele PerceptronMonoCouche pour le batch:  tensor([[0.0104],\n",
      "        [0.0092],\n",
      "        [0.0045],\n",
      "        [0.0124],\n",
      "        [0.0026],\n",
      "        [0.0119],\n",
      "        [0.0098],\n",
      "        [0.0042],\n",
      "        [0.0137],\n",
      "        [0.0150]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#batch de taille 10\n",
    "xbatch = torch.rand(10,3)\n",
    "print(\"input batch\",xbatch,xbatch.size())\n",
    "out = model(xbatch)\n",
    "print(\"prediction du modele PerceptronMonoCouche pour le batch: \",out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Construction du réseau de neurones pour Fashion-MNIST\n",
    "\n",
    "Pour concevoir le réseau *feed forward* pour traiter les données du dataset MNIST, il faut :\n",
    "* connaitre la taille des données d'entrées: les images MNIST sont de taille `28x28` pixels.  Pour passer une image en entrée du réseau, l'image 2D doit être convertit en un tenseur 1D (applatit , *flattened*), ici de taille $28 x 28 = 784$.  Les batchs récupérés sont de taille `64`,  soit un tenseur `(64,1,28,28)`pour 64 images par batch, 1 color channel, et la taille des images 28x28. Dans le cas de batch d'images, des tenseurs 2D peuvent être passés au réseau $(64,784)$.\n",
    "\n",
    "* connaitre la taille de la sortie: le problème est ici un problème de classification en 10 classes (10 types de vêtements). La taille de la sortie du réseau sera donc `10`, 1 sortie par classe. \n",
    "\n",
    "* décider des fonctions d'activation à appliquer: pour les couches cachées, la fonction d'activation ReLU est choisie.\n",
    "\n",
    "* pour la couche de sortie,  on souhaite que la `i`-ème sortie calcule la probabilité que l'image passée  en entrée soit de la classe `i`. Pour convertir la sortie du réseau en distribution de probabilité, la fonction d'activation [**softmax**](https://en.wikipedia.org/wiki/Softmax_function) peut être utilisée car elle permet de *formater* les 10 valeurs de sortie pour qu'elles soient entre 0 et 1 avec une somme égale à 1. La [log-probability](https://en.wikipedia.org/wiki/Log_probability), qui est une fonction qui calcule le logarithme de la fonction softmax, est à préférer car elle apporte différents avantages: les calculs sont souvent plus rapide et plus précis (cf. illustration [ici](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)). Pour récupérer les probabilités à partir de la log-probabilité, il suffit de prendre l'exponentiel (`torch.exp`), en effet:\n",
    "$$ \\large{e^{\\ln{x}} = x }$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille du batch initial:  torch.Size([64, 1, 28, 28])\n",
      "taille du batch final:  torch.Size([64, 784])\n",
      "torch.Size([784])\n"
     ]
    }
   ],
   "source": [
    "#recuperation d'un batch\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"taille du batch initial: \",images.size())\n",
    "\n",
    "#aplatissement des images du batch en 1D\n",
    "images.resize_(images.shape[0], 784) \n",
    "print(\"taille du batch final: \",images.size())\n",
    "\n",
    "print(images[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous proposons de construire pour débuter le réseau suivant (avec `log_softmax` en sortie et pas `softmax`):\n",
    "\n",
    "<img src=\"img/mlp_mnist2.png\" width=600px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Exercice:** Construisez ce réseau. Instanciez le dans une variable `model`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "\n",
    "On va afficher les prédictions du modèle (probabilité de chaque classe) en passant une unique image au réseau:\n",
    "\n",
    "> **Exercice:** Compléter le code ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exp(): argument 'input' (position 1) must be Tensor, not ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-07f75a000dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Calcul des proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Visualisation des prédictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: exp(): argument 'input' (position 1) must be Tensor, not ellipsis"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 784) \n",
    "\n",
    "# FRécupération de la première image du batch\n",
    "img_idx = 0 \n",
    "img = images[img_idx]\n",
    "\n",
    "# TODO: forward pass du modele\n",
    "output = ...\n",
    "\n",
    "\n",
    "\n",
    "# Calcul des proba\n",
    "probs = torch.exp(output)\n",
    "\n",
    "# Visualisation des prédictions\n",
    "helper.view_classify(img.view(1, 28, 28), probs, version='Fashion') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercice:** Que pouvez-vous en conclure ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Entrainement du NN\n",
    "\n",
    "Pour entrainer le réseau de neurones, nous devons :\n",
    "*  définir la fonction de perte à utiliser, dans notre cas, la fonction de perte [**Logarithme Négatif**](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) est adapté à notre problème de classification et à la sortie log-softmax du réseau.\n",
    "\n",
    "* définir l'optimiseur à utiliser, qui se chargera de mettre à jour les paramètres/poids du réseau. Dans notre cas, nous pouvons utiliser la descente de gradient stochastique (SGD) avec `torch.optim.SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement pas à pas\n",
    "\n",
    "Nous allons tout d'abord considérer *une seule itération d'apprentissage*, i.e. une seule mise à jour des paramètres en utilisant un mini-batch de 64 images. Les 4 étapes pour la mise à jour des paramètres sont les suivantes:\n",
    "\n",
    "* *forward pass*: calcul des prédictions du modèle \n",
    "* calcul de la *loss*\n",
    "* *backward pass*: calcul du gradient \n",
    "* mise à jour des paramètres (poids du réseau)\n",
    "\n",
    "> **Exercice:** Implémentez ci-dessous une itération de descente de gradient sur un mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids initiaux -  Parameter containing:\n",
      "tensor([[-0.0032,  0.0040,  0.0137]], requires_grad=True)\n",
      "Gradient - None\n",
      "Poids mis à jour -  Parameter containing:\n",
      "tensor([[-0.0032,  0.0040,  0.0137]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Poids initiaux - ', model.fc1.weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "\n",
    "#TODO\n",
    "\n",
    "\n",
    "\n",
    "##Apres une backward pass\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "print('Poids mis à jour - ', model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement itératif sur plusieurs *epochs*\n",
    "\n",
    "\n",
    "> **Exercice:** Implémentez ci-dessous l'apprentissage complet du réseau de neurones sur plusieurs **epochs**. Vous afficherez toutes les 40 itérations:\n",
    "* le numéro de l'epoch\n",
    "* la perte d'entrainement,qui est la somme des pertes sur ces 40 itérations divisé par 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repet: 1/5..  affiche:: 0.200.. \n",
      "repet: 2/5..  affiche:: 1.200.. \n",
      "repet: 3/5..  affiche:: 2.200.. \n",
      "repet: 4/5..  affiche:: 3.200.. \n",
      "repet: 5/5..  affiche:: 4.200.. \n"
     ]
    }
   ],
   "source": [
    "# exemple pour un affichage dans une boucle\n",
    "repets = 5\n",
    "for i in range(repets):\n",
    "    print(\"repet: {}/{}.. \".format(i+1, repets),\n",
    "         \"affiche:: {:.3f}.. \".format(i+1/repets))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des prédictions sur une image\n",
    "\n",
    "> Exercice: Compléter le code ci-dessous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exp(): argument 'input' (position 1) must be Tensor, not ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-550672bc6846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Calcul des proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Visualisation des prédictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: exp(): argument 'input' (position 1) must be Tensor, not ellipsis"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "\n",
    "# TODO\n",
    "\n",
    "\n",
    "\n",
    "# Calcul des proba\n",
    "probs = torch.exp(output)\n",
    "\n",
    "# Visualisation des prédictions\n",
    "helper.view_classify(img.resize_(1, 28, 28), probs, version='Fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test du réseau et calcul de la précision (*accuracy*)\n",
    "\n",
    "Maintenant que le réseau a été entrainé, nous pouvons tester ses performances. Pour cela, nous pouvons utiliser la perte (*loss*) et/ou la précision (*accuracy*) du réseau, qui correspond au pourcentage de prédictions justes sur l'ensemble des prédictions réalisées.\n",
    "\n",
    "* La précision sur les données d'entrainement permet d'évaluer le sous-apprentissage (*underfit*). Si la précision sur les données d'entrainement est faible, le modèle échoue à apprendre la fonction de prédiction. Dans ce cas, les solutions peuvent être de modifier l'architecture du réseau (par ex. pour un réseau plus grand) ou d'entrainer le réseau plus longtemps. \n",
    "\n",
    "* La précision sur les données de test permet de vérifier si le réseau est capable de **généraliser**, i.e. de réaliser des prédictions sur des données qu'il n'a jamais vu. Si la précision sur les données de test est faible (et que la précision sur les données d'entrainement est élevée), le réseau éhcoue à généraliser (sur-apprentissage, *overfit*).  Dans ce cas, les solutions peuvent être de d'utiliser plus de données d'entrainement, de diminuer le nombre de paramètres, ou de faire de la [**régularisation**](https://medium.com/yottabytes/a-quick-guide-on-basic-regularization-methods-for-neural-networks-e10feb101328), par ex. le *dropout*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercice**: Calculer et afficher ci-dessous la précision de votre réseau sur les données d'entrainement et sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5d7941aa86a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# affichage des précision (en pourcentage)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m print('Précision du réseau sur les %d images d entrainement: %d %%' % (total_train,\n\u001b[0m\u001b[1;32m      5\u001b[0m     accuracy_train))\n\u001b[1;32m      6\u001b[0m print('Précision du réseau sur les %d images de test: %d %%' % (total_test,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_train' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "# affichage des précision (en pourcentage)\n",
    "print('Précision du réseau sur les %d images d entrainement: %d %%' % (total_train,\n",
    "    accuracy_train))\n",
    "print('Précision du réseau sur les %d images de test: %d %%' % (total_test,\n",
    "    accuracy_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde d'un réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est utile de pouvoir sauvegarder un réseau entrainé pour le recharger plus tard (afin de plus l'entrainer, ou de l'utiliser pour faire des prédictions).  Les paramètres d'un réseau PyTorch sont dans `state_dict`. Cette variable contient les poids et biais de chaque couche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " PerceptronMonoCouche(\n",
      "  (fc1): Linear(in_features=3, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['fc1.weight', 'fc1.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le plus simple pour sauvegarder un réseau est de sauvegarder `state_dict` avec `torch.save`, par exemple dans un fichier `checkpoint.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ensuite charger  `state_dict` avec `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['fc1.weight', 'fc1.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode fonctionne si l'architecture du modèle dans lequel on chargera les poids et biais est identique à celle du réseau sauvegardé. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerceptronMonoCouche(\n",
      "  (fc1): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = PerceptronMonoCouche()\n",
    "model.load_state_dict(state_dict)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Compte-rendu\n",
    "\n",
    "Vous pouvez maintenant modifier des éléments pour améliorer la précision de votre réseau si besoin !\n",
    "\n",
    "> Exercice: Préciser ci-dessous les difficultés rencontrés pendant ce TP, les précisions d'entrainement et de test que vous avez obtenu avec le réseau proposé, et les modifications que vous avez peut être testées pour améliorer l'apprentissage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
